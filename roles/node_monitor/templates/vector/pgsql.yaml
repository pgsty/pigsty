# PostgreSQL ecosystem log collector

sources:
  # PostgreSQL CSV logs
  pglog_raw:
    type: file
    read_from: {{ vector_read_from|default('beginning') }}
    include:
      - {{ pg_log_dir }}/*.csv

{% if patroni_enabled|default(true) %}
  # Patroni logs
  patroni_raw:
    type: file
    read_from: {{ vector_read_from|default('beginning') }}
    include:
      - {{ patroni_log_dir }}/patroni.log
{% endif %}
  
{% if pgbackrest_enabled|default(true) %}
  # pgBackRest logs
  # use device_and_inode to avoid fingerprint collision (all pgbackrest logs start with same header)
  pgbackrest_raw:
    type: file
    read_from: {{ vector_read_from|default('beginning') }}
    include:
      - {{ pgbackrest_log_dir }}/*.log
    fingerprint:
      strategy: device_and_inode
{% endif %}

{% if pgbouncer_enabled|default(true) %}
  # pgBouncer logs
  pgbouncer_raw:
    type: file
    read_from: {{ vector_read_from|default('beginning') }}
    include:
      - {{ pgbouncer_log_dir }}/pgbouncer.log
{% endif %}

  # pg_exporter logs (collected via /pg/log/exporter/*.log)
  pgexporter_raw:
    type: file
    read_from: {{ vector_read_from|default('beginning') }}
    include:
      - /pg/log/exporter/*.log

{% if pg_vip_enabled|default(false) %}
  # vip-manager logs (from journald)
  vip_manager_raw:
    type: file
    read_from: {{ vector_read_from|default('beginning') }}
    include:
      - /pg/log/vip-manager.log
{% endif %}

transforms:
  # parse PostgreSQL CSV log
  pglog:
    type: remap
    inputs: [pglog_raw]
    drop_on_abort: true
    source: |
      row = parse_csv!(.message)
      if length(row) < 23 { abort }
      # PostgreSQL timestamp: "2025-12-08 05:05:47.123456+00" -> RFC3339
      ts_str = row[0]
      ts = parse_timestamp(ts_str, "%Y-%m-%d %H:%M:%S%.f%#z") ?? null
      if ts != null {
        ._time = format_timestamp!(ts, "%Y-%m-%dT%H:%M:%S%.fZ")
      } else {
        # fallback to vector's ingestion timestamp
        ._time = format_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
      }
      .username = row[1]
      .datname = row[2]
      .pid = row[3]
      .conn = row[4]
      .sid = row[5]
      .cmd_tag = row[7]
      .level = row[11]
      .code = row[12]
      .msg = row[13]
      .detail = row[14]
      .hint = row[15]
      .context = row[18]
      .query = row[19]
      .appname = row[22]
      del(.message); del(.source_type); del(.host)

  # logs for vlogs
  pglog_logs:
    type: remap
    inputs: [pglog]
    source: |
      .job = "pgsql"
      .ip = "{{ inventory_hostname }}"
      .src = "postgres"
      .ins = "{{ pg_cluster }}-{{ pg_seq }}"
      .cls = "{{ pg_cluster }}"
      .message = truncate(string!(.msg), 4096)
      if .detail != "" { .message = .message + " DETAIL: " + truncate(string!(.detail), 1024) }
      if .hint != "" { .message = .message + " HINT: " + truncate(string!(.hint), 256) }
      if .context != "" { .message = .message + " CONTEXT: " + truncate(string!(.context), 512) }
      if .query != "" { .message = .message + " QUERY: " + truncate(string!(.query), 1024) }
      del(.msg); del(.detail); del(.hint); del(.context); del(.query)
      del(.pid); del(.sid); del(.conn)

{% if patroni_enabled|default(true) %}
  # parse patroni log: "2024-01-01 12:00:00 +0800 INFO: message"
  patroni_logs:
    type: remap
    inputs: [patroni_raw]
    drop_on_abort: true
    source: |
      m = parse_regex(.message, r'^(?P<ts>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} [+-]\d{4}) (?P<level>\w+): (?P<msg>.*)$') ?? {}
      if !exists(m.ts) { abort }
      # Patroni timestamp: "2025-12-08 05:05:47 +0000" -> RFC3339
      ts = parse_timestamp(m.ts, "%Y-%m-%d %H:%M:%S %z") ?? null
      if ts != null {
        ._time = format_timestamp!(ts, "%Y-%m-%dT%H:%M:%S%:z")
      } else {
        # fallback to vector's ingestion timestamp
        ._time = format_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
      }
      .level = m.level
      .message = m.msg
      .job = "pgsql"
      .ip = "{{ inventory_hostname }}"
      .src = "patroni"
      .ins = "{{ pg_cluster }}-{{ pg_seq }}"
      .cls = "{{ pg_cluster }}"
      del(.source_type); del(.host)
{% endif %}

{% if pgbackrest_enabled|default(true) %}
  # parse pgbackrest log: "2024-01-01 12:00:00.123 P00  INFO: message" (timestamp may be optional)
  pgbackrest_logs:
    type: remap
    inputs: [pgbackrest_raw]
    source: |
      # save original message for fallback (e.g. separator lines like "---PROCESS START---")
      original = string!(.message)
      .job = "pgsql"
      .ip = "{{ inventory_hostname }}"
      .src = "pgbackrest"
      .ins = "{{ pg_cluster }}-{{ pg_seq }}"
      .cls = "{{ pg_cluster }}"
      m = parse_regex(original, r'^(?P<ts>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) P\d+\s+(?P<level>\w+):\s*(?P<msg>.*)$') ?? {}
      if exists(m.ts) {
        # pgbackrest timestamp: "2024-01-01 12:00:00.123" -> RFC3339 (assume UTC)
        ts = parse_timestamp(m.ts, "%Y-%m-%d %H:%M:%S%.f") ?? null
        if ts != null {
          ._time = format_timestamp!(ts, "%Y-%m-%dT%H:%M:%S%.fZ")
        } else {
          ._time = format_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
        }
        .level = m.level
        .message = m.msg
      } else {
        # no timestamp in log line, use vector's timestamp
        ._time = format_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
        m = parse_regex(original, r'^P\d+\s+(?P<level>\w+):\s*(?P<msg>.*)$') ?? {}
        if exists(m.level) {
          .level = m.level
          .message = m.msg
        } else {
          # unrecognized format (separator lines, etc.) - use original as message
          .message = original
          .level = "DEBUG"
        }
      }
      del(.source_type); del(.host)
{% endif %}

{% if pgbouncer_enabled|default(true) %}
  # parse pgbouncer log: "2024-01-01 12:00:00.000 TZ [pid] LEVEL message"
  pgbouncer_logs:
    type: remap
    inputs: [pgbouncer_raw]
    drop_on_abort: true
    source: |
      m = parse_regex(.message, r'^(?P<ts>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) (?P<tz>\w+) \[(?P<pid>\d+)\] (?P<level>\w+) (?P<msg>.*)$') ?? {}
      if !exists(m.ts) { abort }
      # pgbouncer timestamp: "2024-01-01 12:00:00.000 UTC" -> RFC3339 (assume UTC)
      ts = parse_timestamp(m.ts, "%Y-%m-%d %H:%M:%S%.f") ?? null
      if ts != null {
        ._time = format_timestamp!(ts, "%Y-%m-%dT%H:%M:%S%.fZ")
      } else {
        # fallback to vector's ingestion timestamp
        ._time = format_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
      }
      .level = m.level
      .message = m.msg
      .job = "pgsql"
      .ip = "{{ inventory_hostname }}"
      .src = "pgbouncer"
      .ins = "{{ pg_cluster }}-{{ pg_seq }}"
      .cls = "{{ pg_cluster }}"
      del(.source_type); del(.host)
{% endif %}

  # pg_exporter/pgbouncer_exporter logs (logrus format, keep simple)
  pgexporter_logs:
    type: remap
    inputs: [pgexporter_raw]
    source: |
      # use vector's timestamp for exporter logs
      ._time = format_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
      .job = "pgsql"
      .ip = "{{ inventory_hostname }}"
      .src = "pgexporter"
      .ins = "{{ pg_cluster }}-{{ pg_seq }}"
      .cls = "{{ pg_cluster }}"
      del(.source_type); del(.host)

{% if pg_vip_enabled|default(false) %}
  # vip-manager logs
  vip_manager_logs:
    type: remap
    inputs: [vip_manager_raw]
    source: |
      # use vector's timestamp for vip-manager logs
      ._time = format_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ")
      .job = "pgsql"
      .ip = "{{ inventory_hostname }}"
      .src = "vip-manager"
      .ins = "{{ pg_cluster }}-{{ pg_seq }}"
      .cls = "{{ pg_cluster }}"
      del(.source_type); del(.file); del(.host)
{% endif %}

{% raw %}
  # metric: log count by level
  pglog_count_metrics:
    type: log_to_metric
    inputs: [pglog]
    metrics:
      - type: counter
        field: level
        name: logs_total
        namespace: pg
        tags:
          level: "{{level}}"

  # extract slow query duration
  pglog_slow:
    type: remap
    inputs: [pglog]
    drop_on_abort: true
    source: |
      if .level != "LOG" { abort }
      m = parse_regex(.msg, r'duration: (?P<ms>\d+\.\d+) ms') ?? {}
      if !exists(m.ms) { abort }
      .duration_ms = to_float!(m.ms)

  # metric: slow query histogram
  pglog_slow_metrics:
    type: log_to_metric
    inputs: [pglog_slow]
    metrics:
      - type: histogram
        field: duration_ms
        name: query_rt_milliseconds
        namespace: pg
        buckets: [1, 10, 100, 500, 1000, 2500, 5000, 10000, 30000]

  # classify errors
  pglog_error:
    type: remap
    inputs: [pglog]
    drop_on_abort: true
    source: |
      if .level != "ERROR" { abort }
      msg = string!(.msg)
      if contains(msg, "statement timeout") { .err = "timeout" } else if contains(msg, "deadlock detected") { .err = "deadlock" } else if contains(msg, "duplicate key") { .err = "dup_key" } else if contains(msg, "transaction is aborted") { .err = "txn_aborted" } else if contains(msg, "canceling autovacuum") { .err = "autovacuum" } else { .err = "other" }

  # metric: error count by type
  pglog_error_metrics:
    type: log_to_metric
    inputs: [pglog_error]
    metrics:
      - type: counter
        field: err
        name: errors_total
        namespace: pg
        tags:
          error: "{{err}}"

  # detect lock waits
  pglog_lock:
    type: remap
    inputs: [pglog]
    drop_on_abort: true
    source: |
      if .level != "LOG" { abort }
      m = parse_regex(.msg, r'still waiting for (?P<mode>\w+) on (?P<type>\w+)') ?? {}
      if !exists(m.mode) { abort }
      .lock_mode = m.mode
      .lock_type = m.type

  # metric: lock wait count
  pglog_lock_metrics:
    type: log_to_metric
    inputs: [pglog_lock]
    metrics:
      - type: counter
        field: lock_mode
        name: lock_waits_total
        namespace: pg
        tags:
          mode: "{{lock_mode}}"
          type: "{{lock_type}}"

  # detect temp file usage
  pglog_temp:
    type: filter
    inputs: [pglog]
    condition: .level == "LOG" && contains(string!(.msg), "temporary file:")

  # metric: temp file count
  pglog_temp_metrics:
    type: log_to_metric
    inputs: [pglog_temp]
    metrics:
      - type: counter
        field: msg
        name: temp_files_total
        namespace: pg

  # detect auth failures
  pglog_auth:
    type: filter
    inputs: [pglog]
    condition: .level == "FATAL" && contains(string!(.msg), "authentication failed")

  # metric: auth failure count
  pglog_auth_metrics:
    type: log_to_metric
    inputs: [pglog_auth]
    metrics:
      - type: counter
        field: username
        name: auth_failures_total
        namespace: pg
        tags:
          user: "{{username}}"
          datname: "{{datname}}"
{% endraw %}
